{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4e0496-caa7-48c2-91d2-e5f1c41c5f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.16\" 2025-07-15\n",
      "OpenJDK Runtime Environment (build 17.0.16+8-Ubuntu-0ubuntu122.04.1)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.16+8-Ubuntu-0ubuntu122.04.1, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674073a8-bf76-410d-ba67-3d129717bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/30 15:41:48 WARN Utils: Your hostname, nadhifa, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/30 15:41:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/30 15:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7457d0cc1550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d96349-bf21-40d5-8982-c1a58e44c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/30 15:50:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/11/30 15:50:32 WARN Instrumentation: [6ca14445] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/30 15:50:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/30 15:50:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2a30ec-d3ff-45d9-9c26-2fc3d6dee6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Features: vector (nullable = true)\n",
      " |-- Label: double (nullable = true)\n",
      "\n",
      "+---+---------+-----+\n",
      "|ID |Features |Label|\n",
      "+---+---------+-----+\n",
      "|1  |[2.0,3.0]|0.0  |\n",
      "|2  |[1.0,5.0]|1.0  |\n",
      "|3  |[2.5,4.5]|1.0  |\n",
      "|4  |[3.0,6.0]|0.0  |\n",
      "+---+---------+-----+\n",
      "\n",
      "Coefficients: [-9.352581579369746,3.1168035677591837]\n",
      "Intercept: 8.66316853737857\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Data: gunakan Vectors.dense untuk kolom fitur, label harus numeric (float atau int)\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0.0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1.0),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1.0),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0.0)\n",
    "]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label', maxIter=20)\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and intercept\n",
    "print('Coefficients:', model.coefficients)\n",
    "print('Intercept:', model.intercept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3739de15-c2eb-4165-be0e-62f17c7888ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Features: vector (nullable = true)\n",
      "\n",
      "+---+-----------+\n",
      "|ID |Features   |\n",
      "+---+-----------+\n",
      "|1  |[1.0,1.0]  |\n",
      "|2  |[5.0,5.0]  |\n",
      "|3  |[10.0,10.0]|\n",
      "|4  |[15.0,15.0]|\n",
      "+---+-----------+\n",
      "\n",
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])),\n",
    "    (2, Vectors.dense([5.0, 5.0])),\n",
    "    (3, Vectors.dense([10.0,10.0])),\n",
    "    (4, Vectors.dense([15.0,15.0]))\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"ID\", \"Features\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "print(\"Cluster Centers:\", model.clusterCenters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bde10de-3935-4322-a321-6a689707c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/30 16:19:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Titanic-MLlib-Homework\").getOrCreate()\n",
    "\n",
    "# Imports umum\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05dbc7b1-a0fb-401a-b7f2-246536289aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 5 rows\n",
      "+----------------------------------------------+-----+\n",
      "|features                                      |label|\n",
      "+----------------------------------------------+-----+\n",
      "|(10,[0,2,3,5,6],[3.0,22.0,1.0,7.25,1.0])      |0.0  |\n",
      "|[1.0,1.0,38.0,1.0,0.0,71.2833,0.0,1.0,0.0,0.0]|1.0  |\n",
      "|(10,[0,1,2,5,6],[3.0,1.0,26.0,7.925,1.0])     |1.0  |\n",
      "|[1.0,1.0,35.0,1.0,0.0,53.1,1.0,0.0,0.0,0.0]   |1.0  |\n",
      "|(10,[0,2,5,6],[3.0,35.0,8.05,1.0])            |0.0  |\n",
      "+----------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "Total rows after prepare: 714\n"
     ]
    }
   ],
   "source": [
    "# Homework 1 oad a real-world dataset into Spark and prepare it for machine learning tasks.\n",
    "# 1. Load Dataset\n",
    "path = \"/home/nadhifa/Downloads/archive/Titanic-Dataset.csv\"\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "# 2. Pilih kolom relevan dan cek\n",
    "df = df.select(\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\")\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# 3. Tangani missing: drop atau imputasi sederhana\n",
    "# Untuk praktikum sederhana kita hapus baris yang punya Age atau Fare null\n",
    "df = df.dropna(subset=['Age','Fare'])\n",
    "\n",
    "# Untuk Embarked yang sedikit missing, imputasi modus:\n",
    "most_freq = df.groupBy(\"Embarked\").count().orderBy(col(\"count\").desc()).first()['Embarked']\n",
    "df = df.fillna({'Embarked': most_freq})\n",
    "\n",
    "# 4. Encode kategori: Sex dan Embarked\n",
    "sex_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndex', handleInvalid='keep')\n",
    "emb_indexer = StringIndexer(inputCol='Embarked', outputCol='EmbIndex', handleInvalid='keep')\n",
    "\n",
    "# 5. (Optional) OneHot jika mau: contoh pakai OneHotEncoder untuk Embarked\n",
    "emb_ohe = OneHotEncoder(inputCols=['EmbIndex'], outputCols=['EmbOHE'], handleInvalid='keep')\n",
    "\n",
    "# 6. Buat features vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pclass','SexIndex','Age','SibSp','Parch','Fare','EmbOHE'],\n",
    "    outputCol='features', handleInvalid='keep'\n",
    ")\n",
    "\n",
    "# 7. Pipeline preprocessing\n",
    "pipeline_prep = Pipeline(stages=[sex_indexer, emb_indexer, emb_ohe, assembler])\n",
    "prep_model = pipeline_prep.fit(df)\n",
    "df_prepared = prep_model.transform(df)\n",
    "\n",
    "# 8. Final dataset untuk modelling: features + label (Pastikan label numeric double)\n",
    "df_ml = df_prepared.select(col('features'), col('Survived').cast('double').alias('label'))\n",
    "df_ml.show(5, truncate=False)\n",
    "print(\"Total rows after prepare:\", df_ml.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a5d120-4fac-48f7-92ee-41e97639eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (ROC): 0.9048\n",
      "Accuracy: 0.7982\n",
      "\n",
      "Confusion counts (label, prediction, count):\n",
      "   label  prediction  count\n",
      "0    1.0         1.0     38\n",
      "1    0.0         1.0     10\n",
      "2    1.0         0.0     13\n",
      "3    0.0         0.0     53\n"
     ]
    }
   ],
   "source": [
    "# Homework 2 Build a classification model using Spark MLlib and evaluate its performance.\n",
    "\n",
    "# train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Definisikan model (Logistic Regression)\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=50)\n",
    "\n",
    "# 3. Fit model\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# 4. Prediksi di test\n",
    "pred = lr_model.transform(test)\n",
    "\n",
    "# 5. Evaluator: AUC (binary) dan accuracy (multiclass evaluator)\n",
    "bce = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = bce.evaluate(pred)\n",
    "mce = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = mce.evaluate(pred)\n",
    "\n",
    "print(f\"AUC (ROC): {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 6. Confusion matrix kecil (kumpulkan ke pandas untuk tampilan)\n",
    "cm = pred.groupBy('label','prediction').count().toPandas()\n",
    "print(\"\\nConfusion counts (label, prediction, count):\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037cd5aa-3eca-4a87-97b1-4e35aba95200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RegParam: 0.01\n",
      "Best ElasticNetParam: 1.0\n",
      "AUC after CV: 0.9038, Accuracy after CV: 0.7895\n"
     ]
    }
   ],
   "source": [
    "# Homework 3 Explore hyperparameter tuning using cross-validation.\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# 1. Siapkan estimator & evaluator\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=50)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "\n",
    "# 2. Buat grid parameter\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "\n",
    "# 3. CrossValidator (gunakan numFolds=3 atau 4 sesuai waktu)\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    parallelism=2)  # parallelism = berapa model yang dilatih paralel\n",
    "\n",
    "# 4. Fit CV (hati-hati: ini butuh waktu)\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# 5. Prediksi & evaluasi model terbaik\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"Best RegParam:\", bestModel._java_obj.getRegParam())\n",
    "print(\"Best ElasticNetParam:\", bestModel._java_obj.getElasticNetParam())\n",
    "\n",
    "pred_cv = cvModel.transform(test)\n",
    "auc_cv = evaluator.evaluate(pred_cv)\n",
    "acc_cv = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(pred_cv)\n",
    "print(f\"AUC after CV: {auc_cv:.4f}, Accuracy after CV: {acc_cv:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8df3fc-1dc4-4af7-a809-6814442f0816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
